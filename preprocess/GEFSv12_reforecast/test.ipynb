{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8180d802-c32c-4204-b50e-e7b3ffd06378",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import libraries\n",
    "import os, sys\n",
    "import yaml\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "# path_to_repo = '/home/dnash/repos/SEAK_AR_impacts/'\n",
    "# sys.path.append(path_to_repo+'modules')\n",
    "# import GEFSv12_funcs as gefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15e01b8c-cf0a-4286-80df-884156c3f028",
   "metadata": {},
   "outputs": [],
   "source": [
    "variable = 'prec'\n",
    "year = 2000\n",
    "date = 20000101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bf870d5-fdca-442b-afc2-3d21a9dddccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data = \"/expanse/lustre/scratch/dnash/temp_project/downloaded/GEFSv12_reforecast/{0}/\".format(date)\n",
    "\n",
    "## read the control\n",
    "fname = \"apcp_sfc_{0}00_c00.grib2\".format(date)\n",
    "dsa = xr.open_dataset(path_to_data+fname, engine='cfgrib',filter_by_keys={'dataType': 'cf'})\n",
    "dsa = dsa.expand_dims(\"number\") ## expand the dim so we can concat with the other data\n",
    "\n",
    "## read the ensemble members\n",
    "fname_pattern = \"apcp_sfc_{0}00_p*.grib2\".format(date)\n",
    "dsb = xr.open_mfdataset(path_to_data+fname_pattern, engine='cfgrib', \n",
    "                        concat_dim=\"number\", combine='nested',filter_by_keys={'dataType': 'pf'})\n",
    "\n",
    "## concat into single ds\n",
    "ds = xr.concat([dsa, dsb], dim='number', coords='minimal', compat='override')\n",
    "\n",
    "## fix lons \n",
    "ds = ds.assign_coords({\"longitude\": (((ds.longitude + 180) % 360) - 180)}) # Convert DataArray longitude coordinates from 0-359 to -180-179\n",
    "\n",
    "## subset to N. America [0, 70, 180, 295]\n",
    "ds = ds.sel(latitude=slice(70, 0), longitude=slice(-179.5, -60.))\n",
    "\n",
    "## convert precipitation to mm per hour\n",
    "ts_3hr = pd.timedelta_range(start='0 day', periods=57, freq='3H')\n",
    "ts_6hr = pd.timedelta_range(start='0 day', periods=29, freq='6H')\n",
    "tp = ds.tp ## pull out tp\n",
    "prec_3hr = tp.sel(step=ts_3hr[1::2]) ## grab only the 3hr values\n",
    "tp2 = tp.diff(dim='step') ## calculate difference in precip\n",
    "## the values for 6hr timesteps are correct, the values for 3hr timesteps are incorrect\n",
    "prec_6hr = tp2.sel(step=ts_6hr[1:]) # grab only the 6hr values\n",
    "new_prec = prec_3hr.combine_first(prec_6hr) # combine the correct 3hr values with the correct 6hr values\n",
    "ds = ds.drop_vars([\"tp\"]) # get rid of old tp (accumulated variable)\n",
    "ds = xr.merge([ds, new_prec]) # merge dataset with new tp\n",
    "\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086f5a2b-9c8b-482b-adae-a5281bc278de",
   "metadata": {},
   "outputs": [],
   "source": [
    "## save data to netCDF file\n",
    "print('Writing {0} to netCDF ....'.format(date))\n",
    "path_to_data = '/expanse/lustre/scratch/dnash/temp_project/mclimate/{0}/'.format(variable)\n",
    "\n",
    "fname = '{0}_{1}.nc'.format(date, variable) \n",
    "ds.load().to_netcdf(path=path_to_data + fname, mode = 'w', format='NETCDF4')\n",
    "\n",
    "### COPY FROM LUSTRE TO MAIN SPACE\n",
    "path_to_final_data = '/expanse/nfs/cw3e/cwp140/preprocessed/GEFSv12_reforecast/{0}/'.format(variable)\n",
    "print('Copying preprocessed data...')\n",
    "inname = path_to_data + fname\n",
    "outname = path_to_final_data + fname\n",
    "print('... {0} to {1}'.format(inname, outname))\n",
    "shutil.copy(inname, outname) # copy file over to data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84889950-7c9e-45df-a5b9-4d857c6fa578",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.tp.isel(latitude=12, longitude=200, number=4).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9188a282-5e81-4e6b-9298-7237af6b691a",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# Filename:    preprocess_GEFSv12_reforecast.py\n",
    "# Author:      Deanna Nash dnash@ucsd.edu\n",
    "# Description: Script to take downloaded GEFSv12 reforecast u, v, and spfh data for each day, preprocess IVT data and save as single netCDF file\n",
    "# https://registry.opendata.aws/noaa-gefs-reforecast/ (data link)\n",
    "#\n",
    "######################################################################\n",
    "\n",
    "## import libraries\n",
    "import os, sys\n",
    "import yaml\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "\n",
    "path_to_repo = '/home/dnash/repos/SEAK_AR_impacts/'\n",
    "sys.path.append(path_to_repo+'modules')\n",
    "import GEFSv12_funcs as gefs\n",
    "\n",
    "# path_to_data = '/expanse/nfs/cw3e/cwp140/'\n",
    "path_to_data = '/expanse/lustre/scratch/dnash/temp_project/'\n",
    "\n",
    "config_file = 'config_2.yaml' # this is the config file name\n",
    "job_info = 'job_492' # this is the job name\n",
    "\n",
    "config = yaml.load(open(config_file), Loader=yaml.SafeLoader) # read the file\n",
    "ddict = config[job_info] # pull the job info from the dict\n",
    "\n",
    "year = ddict['year']\n",
    "date = ddict['date']\n",
    "variable = 'ivt' ## can be 'ivt', 'freezing_level', or 'uv1000'\n",
    "\n",
    "for i, st in enumerate(range(0, 8, 8)):\n",
    "    print(st, st+8)\n",
    "    start = st\n",
    "    stop = st+8\n",
    "    \n",
    "    if variable == 'ivt':\n",
    "        print('Loading u, v, and q data ....')\n",
    "        varname_lst = ['ugrd', 'vgrd', 'spfh']\n",
    "        ds_lst = []\n",
    "        for i, varname in enumerate(varname_lst):\n",
    "            ds = gefs.read_and_regrid_prs_var(varname, date, year, start, stop)\n",
    "            ds = ds.isel(step=slice(0, 8))\n",
    "            ds_lst.append(ds)\n",
    "\n",
    "    if variable == 'uv1000':\n",
    "        print('Loading u and v data ....')\n",
    "        varname_lst = ['ugrd_pres', 'vgrd_pres']\n",
    "        ds_lst = []\n",
    "        \n",
    "        for i, varname in enumerate(varname_lst):\n",
    "            ds = gefs.read_sfc_var(varname, date, year, start, stop)\n",
    "            ds = ds.sel(isobaricInhPa=1000.)\n",
    "            ds = ds.isel(step=slice(0, 8))\n",
    "            ds_lst.append(ds)\n",
    "        ds = xr.merge(ds_lst) # merge u, v, and q into single ds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d67cc1-9fe6-4bb4-9b87-f0da591181b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c067df1d-4a15-4129-8455-80b14fe9e2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "import numpy as np\n",
    "mon = 11\n",
    "day = 17\n",
    "## for each year between 2000 and 2019\n",
    "date_lst = []\n",
    "for i, yr in enumerate(range(2000, 2020)):\n",
    "    ## get 45 days before date\n",
    "    center_date = '{0}-{1}-{2}'.format(yr, mon, day)\n",
    "    center_date = pd.to_datetime(center_date)\n",
    "    start_date = center_date - timedelta(days=45)\n",
    "    \n",
    "    ## get 45 days after November 21\n",
    "    end_date = center_date + timedelta(days=45)\n",
    "\n",
    "    ## make a list of dates between start_date and end_date\n",
    "    dates = pd.date_range(start_date, end_date, freq='1D')\n",
    "    \n",
    "    date_lst.append(dates)\n",
    "\n",
    "final_lst = np.concatenate(date_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853a0d2d-6a68-44a4-b844-92128fbf84e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load in surface pressure\n",
    "print('Loading surface pressure data ....')\n",
    "ds_pres = gefs.read_sfc_var('pres_sfc', date, year, start, stop)\n",
    "ds_lst.append(ds_pres)\n",
    "\n",
    "ds = xr.merge(ds_lst) # merge u, v, and q into single ds\n",
    "ds = ds.sel(isobaricInhPa=slice(300, 1000))\n",
    "ds = ds.reindex(isobaricInhPa=ds.isobaricInhPa[::-1])\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c1eac5-edee-48e8-ba97-0e07f6d1b65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## mask values below surface pressure\n",
    "print('Masking values below surface ....')\n",
    "varlst = ['q', 'u', 'v']\n",
    "for i, varname in enumerate(varlst):\n",
    "    ds[varname] = ds[varname].where(ds[varname].isobaricInhPa < ds.sp/100., drop=False)\n",
    "\n",
    "## integrate to calculate IVT\n",
    "print('Calculating IVT ....')\n",
    "ds_IVT = gefs.calc_IVT_manual(ds) # calculate IVT\n",
    "ds_IVT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46d0e43-d291-4ef5-b04c-108a176d3ab6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3b7d6b-83b4-40e2-a71c-33f91b2f6e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = ds_IVT.step.values[0].astype('timedelta64[h]')\n",
    "stop = ds_IVT.step.values[-1].astype('timedelta64[h]')\n",
    "start = int(start / np.timedelta64(1, 'h'))\n",
    "stop = int(stop / np.timedelta64(1, 'h'))\n",
    "print(start, stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a4a994-bc2d-4a51-a65d-311e7f3a5909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get info for saving file\n",
    "start = ds_IVT.step.values[0].astype('timedelta64[h]')\n",
    "stop = ds_IVT.step.values[-1].astype('timedelta64[h]')\n",
    "start = int(start / np.timedelta64(1, 'h'))\n",
    "stop = int(stop / np.timedelta64(1, 'h'))\n",
    "\n",
    "## save IVT data to netCDF file\n",
    "print('Writing {0} to netCDF ....'.format(date))\n",
    "path_to_data = '/expanse/nfs/cw3e/cwp140/'\n",
    "out_fname = path_to_data + 'preprocessed/GEFSv12_reforecast/ivt/{0}_ivt_F{1}_F{2}.nc'.format(date, start, stop) \n",
    "ds_IVT.load().to_netcdf(path=out_fname, mode = 'w', format='NETCDF4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2145c5c0-e456-49dc-9514-359a842029e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "path_to_data = '/expanse/nfs/cw3e/cwp140/'\n",
    "out_fname = path_to_data + 'preprocessed/GEFSv12_reforecast/ivt/20050624_ivt_F123_F144.nc'\n",
    "ds = xr.open_dataset(out_fname)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3993a586-c8f8-4d39-82e6-0d839273c01b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffa0d3a-f830-4383-905e-6398d42a1da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_fname = path_to_data + 'preprocessed/GEFSv12_reforecast/ivt/20050623_ivt_F123_F144.nc'\n",
    "ds2 = xr.open_dataset(out_fname)\n",
    "ds2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea61263d-2b7f-4519-953a-093eac2c57de",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import libraries\n",
    "import os, sys\n",
    "import yaml\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "\n",
    "path_to_repo = '/cw3e/mead/projects/cwp140/scratch/dnash/repos/SEAK_AR_impacts/'\n",
    "sys.path.append(path_to_repo+'modules')\n",
    "import GEFSv12_funcs as gefs\n",
    "\n",
    "path_to_data = '/cw3e/mead/projects/cwp140/scratch/dnash/data/'\n",
    "\n",
    "config_file = 'config_1.yaml' # this is the config file name\n",
    "job_info = 'job_154' # this is the job name\n",
    "\n",
    "config = yaml.load(open(config_file), Loader=yaml.SafeLoader) # read the file\n",
    "ddict = config[job_info] # pull the job info from the dict\n",
    "\n",
    "year = ddict['year']\n",
    "date = ddict['date']\n",
    "variable = 'ivt' ## can be 'ivt', 'freezing_level', or 'prec'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522522ae-46b0-44a6-b033-c24552da3060",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "varname_lst = ['ugrd', 'vgrd', 'spfh']\n",
    "path_to_data = '/cw3e/mead/projects/cwp140/scratch/dnash/data/downloads/GEFSv12_reforecast/{0}/'.format(date) \n",
    "    \n",
    "# read data below 700 mb - 0.25 degree\n",
    "fname_lst = glob.glob(path_to_data+\"{0}_pres_abv700mb_{1}00_*.grib2\".format(varname_lst[2], date))\n",
    "print(fname_lst)\n",
    "\n",
    "# fname_lst = glob.glob(path_to_data+\"{0}_pres_{1}00*.grib2\".format(varname_lst[2], date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de714806-7e60-4c5e-9fa4-c5efdee6cfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(ds, start, stop):\n",
    "    '''keep only the first 24 hours'''\n",
    "    return ds.isel(step=slice(start, stop))\n",
    "    \n",
    "def fix_GEFSv12_open_mfdataset(fname, start, stop):\n",
    "    list_of_files = glob.glob(fname)\n",
    "    ds_lst = []\n",
    "    for i, fi in enumerate(list_of_files):\n",
    "        ds = xr.open_dataset(fi)\n",
    "        if ds['time'].size > 1:\n",
    "            ds = ds.isel(time=0)\n",
    "        \n",
    "        ds_lst.append(ds)\n",
    "\n",
    "    ## get max step size\n",
    "    step_size_lst = []\n",
    "    for i, ds in enumerate(ds_lst):\n",
    "        step_size_lst.append(ds.step.size)\n",
    "    max_size = max(step_size_lst)\n",
    "    max_index = step_size_lst.index(max(step_size_lst))\n",
    "    max_time = ds_lst[max_index].valid_time.values\n",
    "    max_ds = ds_lst[max_index]\n",
    "    ## now loop through and fill ds where smaller than max size\n",
    "    new_ds_lst = []\n",
    "    for i, tmp in enumerate(ds_lst):\n",
    "        if tmp.step.size < max_size:\n",
    "            new_ds = tmp.reindex_like(max_ds, method='nearest', fill_value=np.nan)\n",
    "            # new_ds = new_ds.drop_dims(\"valid_time\")\n",
    "            new_ds = new_ds.assign_coords(valid_time=(\"step\", max_time))\n",
    "            new_ds = preprocess(new_ds, start, stop)\n",
    "            # new_ds = ds.expand_dims(\"valid_time\").assign_coords(valid_time=max_time)\n",
    "            # new_ds = ds.update({\"valid_time\": max_time})\n",
    "            # ds1, new_ds = xr.align(ds_above[max_index], ds, join=\"left\")\n",
    "            new_ds_lst.append(new_ds)\n",
    "    \n",
    "        elif ds.step.size == max_size:\n",
    "            ds = preprocess(ds, start, stop)\n",
    "            new_ds_lst.append(ds)\n",
    "        \n",
    "    ds = xr.concat(new_ds_lst, dim=\"number\")\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd62c0ae-cc77-4441-af83-7c097a1277f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_above = fix_GEFSv12_open_mfdataset_test(fname_lst, 72, 80)\n",
    "ds_above "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:SEAK-impacts]",
   "language": "python",
   "name": "conda-env-SEAK-impacts-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
